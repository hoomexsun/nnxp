{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0083f1f7",
   "metadata": {},
   "source": [
    "# Build source and configuration files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172aa9fc",
   "metadata": {},
   "source": [
    "## 0. Remove existing directories and content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f1db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "conf_root = Path(\"conf\")\n",
    "src_root = Path(\"src\")\n",
    "\n",
    "# Remove existing directories if they exist\n",
    "shutil.rmtree(conf_root, ignore_errors=True)\n",
    "shutil.rmtree(src_root, ignore_errors=True)\n",
    "\n",
    "# Make directories\n",
    "conf_root.mkdir(exist_ok=True)\n",
    "src_root.mkdir(exist_ok=True)\n",
    "Path(\"conf/tuning\").mkdir(exist_ok=True)\n",
    "Path(\"src/tokenizer\").mkdir(exist_ok=True)\n",
    "Path(\"src/data\").mkdir(exist_ok=True)\n",
    "Path(\"src/models\").mkdir(exist_ok=True)\n",
    "Path(\"src/utils\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bc77bc",
   "metadata": {},
   "source": [
    "## 1. Configuration files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492bede2",
   "metadata": {},
   "source": [
    "### 1.1 Default train config (seq2seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb4e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf/train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf/train.yaml\n",
    "xlit: attention\n",
    "xlit_conf:\n",
    "    # Data Setting\n",
    "    langx: ben\n",
    "    langy: mni\n",
    "    token_type: char\n",
    "    db_file: db/transcribed.txt\n",
    "    max_len: 100\n",
    "    val_ratio: 0.25\n",
    "    \n",
    "    # Model Setting\n",
    "    idim: 64\n",
    "    odim: 48\n",
    "    hidden_dim: 128\n",
    "    embed_dim: 128\n",
    "    elayers: 2\n",
    "    dlayers: 2\n",
    "    dropout: 0.25\n",
    "\n",
    "    # Optimizer Setting\n",
    "    optim: adam\n",
    "    optim_conf:\n",
    "        lr: 1.0e-03\n",
    "        eps: 1.0e-06\n",
    "        weight_decay: 0.0\n",
    "    \n",
    "    # Training Setting\n",
    "    max_epoch: 50\n",
    "    batch_size: 32\n",
    "    keep_nbest_models: 5\n",
    "    seed: 248"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f223a4",
   "metadata": {},
   "source": [
    "### 1.2 LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d822d8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf/tuning/lstm.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf/tuning/lstm.yaml\n",
    "xlit: lstm\n",
    "xlit_conf:\n",
    "    langx: ben\n",
    "    langy: mni\n",
    "    token_type: char\n",
    "    db_file: db/transcribed.txt\n",
    "    max_len: 100\n",
    "    val_ratio: 0.25\n",
    "\n",
    "    # Model\n",
    "    idim: 64\n",
    "    odim: 48\n",
    "    hidden_dim: 128\n",
    "    embed_dim: 128\n",
    "    nlayers: 2\n",
    "    dropout: 0.25\n",
    "\n",
    "    # Optimizer\n",
    "    optim: adam\n",
    "    optim_conf:\n",
    "        lr: 1.0e-3\n",
    "        eps: 1.0e-6\n",
    "        weight_decay: 0.0\n",
    "\n",
    "    # Training\n",
    "    max_epoch: 50\n",
    "    batch_size: 32\n",
    "    keep_nbest_models: 5\n",
    "    seed: 248\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8b073",
   "metadata": {},
   "source": [
    "### 1.3 CNN+Position Encoder+GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d905fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf/tuning/cnn.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf/tuning/cnn.yaml\n",
    "# conf/tuning/cnn_attn.yaml\n",
    "xlit: cnn_attn\n",
    "xlit_conf:\n",
    "    langx: ben\n",
    "    langy: mni\n",
    "    token_type: char\n",
    "    db_file: db/transcribed.txt\n",
    "    max_len: 100\n",
    "    val_ratio: 0.25\n",
    "\n",
    "    # Model\n",
    "    idim: 64\n",
    "    odim: 47\n",
    "    embed_dim: 128\n",
    "    hidden_dim: 256\n",
    "    kernel_size: 3\n",
    "    dropout: 0.25\n",
    "    teacher_forcing_ratio: 0.5\n",
    "\n",
    "    # Optimizer\n",
    "    optim: adam\n",
    "    optim_conf:\n",
    "        lr: 1.0e-3\n",
    "        eps: 1.0e-6\n",
    "        weight_decay: 0.0\n",
    "\n",
    "    # Training\n",
    "    max_epoch: 50\n",
    "    batch_size: 32\n",
    "    keep_nbest_models: 5\n",
    "    seed: 248\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80adc139",
   "metadata": {},
   "source": [
    "### 1.4 Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f73acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing conf/tuning/transformer.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile conf/tuning/transformer.yaml\n",
    "xlit: transformer\n",
    "xlit_conf:\n",
    "    langx: ben\n",
    "    langy: mni\n",
    "    token_type: char\n",
    "    db_file: db/transcribed.txt\n",
    "    max_len: 100\n",
    "    val_ratio: 0.25\n",
    "\n",
    "    # Model\n",
    "    idim: 64\n",
    "    odim: 48\n",
    "    embed_dim: 256\n",
    "    num_heads: 4 \n",
    "    num_encoder_layers: 4\n",
    "    num_decoder_layers: 4 \n",
    "    dim_feedforward: 512 \n",
    "    dropout: 0.1\n",
    "\n",
    "    # Optimizer\n",
    "    optim: adam\n",
    "    optim_conf:\n",
    "        lr: 1.0e-4\n",
    "        eps: 1.0e-6\n",
    "        weight_decay: 0.0\n",
    "\n",
    "    # Training\n",
    "    max_epoch: 20\n",
    "    batch_size: 16\n",
    "    keep_nbest_models: 5\n",
    "    seed: 248\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48244a",
   "metadata": {},
   "source": [
    "## 2. Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5443ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/tokenizer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/tokenizer/__init__.py\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "from ..data.utils import load_pairs\n",
    "\n",
    "from .char_tokenizer import CharTokenizer\n",
    "# from .phn_tokenizer import PhonemeTokenizer\n",
    "\n",
    "TOKENIZER_REGISTRY = dict(\n",
    "    char=CharTokenizer,\n",
    "    # phn=PhonemeTokenizer\n",
    ")\n",
    "\n",
    "def _infer_token_type(tokens_file: Path) -> str:\n",
    "    parts = tokens_file.stem.split(\"_\")\n",
    "    if len(parts) < 2:\n",
    "        raise ValueError(f\"Cannot infer token type from file name: {tokens_file.name}\")\n",
    "    return parts[-2]\n",
    "\n",
    "def build_tokenizer(xs: Iterable[str], tokens_file: Path) -> None:\n",
    "    token_type = _infer_token_type(tokens_file)\n",
    "    tokenizer_cls = TOKENIZER_REGISTRY.get(token_type)\n",
    "    if tokenizer_cls is None:\n",
    "        raise ValueError(f\"Unsupported token type: {token_type}\")\n",
    "    \n",
    "    tokenizer = tokenizer_cls(xs)\n",
    "    tokenizer.to_token_file(tokens_file)\n",
    "\n",
    "def load_tokenizer(tokens_file: Path):\n",
    "    if not tokens_file.exists():\n",
    "        raise FileNotFoundError(f\"Tokenizer file not found: {tokens_file.as_posix()}\")\n",
    "    \n",
    "    token_type = _infer_token_type(tokens_file)\n",
    "    tokenizer_cls = TOKENIZER_REGISTRY.get(token_type)\n",
    "    if tokenizer_cls is None:\n",
    "        raise ValueError(f\"Unsupported token type: {token_type}\")\n",
    "    \n",
    "    return tokenizer_cls.from_token_file(tokens_file)\n",
    "\n",
    "def prepare_tokenizers(x_tokens_file, y_tokens_file, db_file):\n",
    "    xs, ys = [], []\n",
    "    try:\n",
    "        x_tokenizer = load_tokenizer(x_tokens_file)\n",
    "        y_tokenizer = load_tokenizer(y_tokens_file)\n",
    "    except FileNotFoundError:\n",
    "        xs, ys = load_pairs(db_file)\n",
    "        build_tokenizer(xs, x_tokens_file)\n",
    "        build_tokenizer(ys, y_tokens_file)\n",
    "        x_tokenizer = load_tokenizer(x_tokens_file)\n",
    "        y_tokenizer = load_tokenizer(y_tokens_file)\n",
    "    return x_tokenizer, y_tokenizer, xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d987c5d",
   "metadata": {},
   "source": [
    "### 2.1 Base Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fde1981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/tokenizer/base.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/tokenizer/base.py\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Tokenizer(ABC):\n",
    "    def __init__(self, vocab: List[str]) -> None:\n",
    "        self.idx2tok = self._create_idx2tok(tokens=vocab)\n",
    "        self.tok2idx = {tok: idx for idx, tok in self.idx2tok.items()}\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _create_idx2tok(self, tokens: List[str]) -> Dict[int, str]:\n",
    "        pass\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tok2idx)\n",
    "    \n",
    "    def encode(self, text: str, max_len: int = 100) -> List[int]: \n",
    "        tokens = (\n",
    "            [self.tok2idx.get(\"<sos>\")]\n",
    "            + [self.tok2idx.get(ch) for ch in text if ch in self.tok2idx]\n",
    "            + [self.tok2idx.get(\"<eos>\")]\n",
    "        )\n",
    "        tokens += [self.tok2idx.get(\"<pad>\")] * (max_len - len(tokens))\n",
    "        return tokens[:max_len]\n",
    "    \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return \"\".join(\n",
    "            self.idx2tok[idx]\n",
    "            for idx in indices\n",
    "            if idx in self.idx2tok and idx not in {\n",
    "                self.tok2idx[\"<pad>\"],\n",
    "                self.tok2idx[\"<sos>\"],\n",
    "                self.tok2idx[\"<eos>\"]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_token_file(cls, path: str | Path) -> \"Tokenizer\":\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Tokenizer file not found: {path.as_posix()}\")\n",
    "        tokenizer = cls(vocab=[])\n",
    "        tokenizer.idx2tok = enum_str(path.read_text(encoding=\"utf-8\"))\n",
    "        tokenizer.tok2idx = {tok: idx for idx, tok in tokenizer.idx2tok.items()}\n",
    "        \n",
    "        return tokenizer\n",
    "\n",
    "    def to_token_file(self, path: str | Path) -> None:\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True) \n",
    "        tokens = [self.idx2tok[i] for i in range(len(self))]\n",
    "        path.write_text(\"\\n\".join(tokens), encoding=\"utf-8\")\n",
    "    \n",
    "    \n",
    "def enum_str(s: str, start: int = 0) -> dict[int, str]:\n",
    "    return {i: tok for i, tok in enumerate(s.strip().splitlines(), start=start)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86616da",
   "metadata": {},
   "source": [
    "### 2.2 Character tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40acfd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/tokenizer/char_tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/tokenizer/char_tokenizer.py\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from .base import Tokenizer\n",
    "\n",
    "class CharTokenizer(Tokenizer):\n",
    "    def __init__(self, vocab: List[str]) -> None:\n",
    "        special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "        tokens = special_tokens + sorted(set(\"\".join(vocab)))\n",
    "        super().__init__(vocab=tokens) \n",
    "\n",
    "    def _create_idx2tok(self, tokens: List[str]) -> dict[int, str]:\n",
    "        return {idx: tok for idx, tok in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c90422",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Manipulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28620e6c",
   "metadata": {},
   "source": [
    "### 3.1 Data Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a502957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/data/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/data/utils.py\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_pairs(pairs: list[tuple], path: str | Path, sep:str=\"\\t\") -> None:\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(\"\\n\".join([f\"{x}{sep}{y}\" for x, y in pairs]), encoding=\"utf-8\")\n",
    "    \n",
    "    \n",
    "def load_pairs(path: str | Path, sep:str=\"\\t\") -> tuple[list, list]:\n",
    "    xs, ys = [], []\n",
    "    for line in Path(path).read_text(encoding=\"utf-8\").strip().split(\"\\n\"):\n",
    "        x,  y, *_ = line.strip().split(sep, maxsplit=1)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23bf95e",
   "metadata": {},
   "source": [
    "### 3.2 XlitDataset and load_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8671a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/data/loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/data/loader.py\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch\n",
    "\n",
    "\n",
    "class XlitDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        xs: List[str],\n",
    "        ys: List[str],\n",
    "        x_tokenizer,\n",
    "        y_tokenizer,\n",
    "        max_len: int,\n",
    "    ) -> None:\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.x_tokenizer = x_tokenizer\n",
    "        self.y_tokenizer = y_tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, str]]:\n",
    "        x, y = self.xs[idx], self.ys[idx]\n",
    "        x_encoded = self.x_tokenizer.encode(x, self.max_len)\n",
    "        y_encoded = self.y_tokenizer.encode(y, self.max_len)\n",
    "        return {\n",
    "            \"input\": torch.tensor(x_encoded, dtype=torch.long),\n",
    "            \"target\": torch.tensor(y_encoded, dtype=torch.long),\n",
    "            \"input_text\": x,\n",
    "            \"target_text\": y,\n",
    "        }\n",
    "\n",
    "    def save_pairs_to_file(\n",
    "        self, save_path: Union[str, Path], indices: Optional[List[int]] = None\n",
    "    ) -> None:\n",
    "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            if indices is None:\n",
    "                iterable = zip(self.xs, self.ys)\n",
    "            else:\n",
    "                iterable = ((self.xs[i], self.ys[i]) for i in indices)\n",
    "            for x, y in iterable:\n",
    "                f.write(f\"{x}\\t{y}\\n\")\n",
    "\n",
    "\n",
    "def load_dataloaders(\n",
    "    xs: List[str],\n",
    "    ys: List[str],\n",
    "    x_tokenizer,\n",
    "    y_tokenizer,\n",
    "    max_len: int = 100,\n",
    "    batch_size: int = 32,\n",
    "    val_ratio: float = 0.25,\n",
    "    train_file: Optional[Union[str, Path]] = None,\n",
    "    val_file: Optional[Union[str, Path]] = None,\n",
    "    seed: Optional[int] = None,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    dataset = XlitDataset(xs, ys, x_tokenizer, y_tokenizer, max_len)\n",
    "    val_size = int(val_ratio * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    if seed is not None:\n",
    "        generator.manual_seed(seed)\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Save data pairs if requested\n",
    "    if train_file:\n",
    "        dataset.save_pairs_to_file(train_file, indices=train_dataset.indices)\n",
    "    if val_file:\n",
    "        dataset.save_pairs_to_file(val_file, indices=val_dataset.indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779747e",
   "metadata": {},
   "source": [
    "## 4. Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48545ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/models/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/__init__.py\n",
    "from .attn import AttnSeq2Seq\n",
    "from .lstm import LSTMSeq2Seq\n",
    "from .cnn import CNNSeq2SeqAttn\n",
    "from .transformer import TransformerSeq2Seq\n",
    "\n",
    "MODEL_REGISTRY = dict(\n",
    "    attention=AttnSeq2Seq,\n",
    "    lstm=LSTMSeq2Seq,\n",
    "    cnn_attn=CNNSeq2SeqAttn,\n",
    "    transformer=TransformerSeq2Seq,\n",
    ")\n",
    "\n",
    "\n",
    "def load_model(model_name, model_conf, device):\n",
    "    if model_name not in MODEL_REGISTRY:\n",
    "        raise ValueError(f\"Model '{model_name}' not supported.\")\n",
    "    model_cls = MODEL_REGISTRY[model_name]\n",
    "    model = model_cls(model_conf, device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769717eb",
   "metadata": {},
   "source": [
    "### 4.0 Base Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8287f1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/models/base.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile src/models/base.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class XlitModel(nn.Module):\n",
    "    def __init__(self, model_conf: dict, device: torch.device) -> None:\n",
    "            super().__init__()\n",
    "            self.device = device\n",
    "            self.max_len = model_conf[\"max_len\"]\n",
    "            self.sos_token = model_conf[\"sos_token\"]\n",
    "            self.eos_token = model_conf[\"eos_token\"]\n",
    "            self.pad_token = model_conf[\"pad_token\"]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb2660",
   "metadata": {},
   "source": [
    "### 4.1. Attention Seq2Seq Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d589672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/models/attn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/attn.py\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from src.models.base import XlitModel\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        embed_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden: torch.Tensor,\n",
    "        encoder_outputs: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        batch_size, seq_len, _ = encoder_outputs.size()\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        combined = torch.cat([hidden, encoder_outputs], dim=2)\n",
    "        energy = torch.tanh(self.attn(combined))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim: int,\n",
    "        embed_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_dim + hidden_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inp: torch.Tensor,\n",
    "        hidden: torch.Tensor,\n",
    "        cell: torch.Tensor,\n",
    "        encoder_outputs: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        inp = inp.unsqueeze(1)  # (batch_size, 1)\n",
    "        embedded = self.dropout(self.embedding(inp))  # (batch_size, 1, embed_dim)\n",
    "        # (batch_size, seq_len)\n",
    "        attn_weights = self.attention(hidden, encoder_outputs, mask)\n",
    "        # (batch_size, 1, hidden_dim)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        # (batch_size, 1, embed+hidden)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        prediction = self.fc_out(\n",
    "            torch.cat((output.squeeze(1), context.squeeze(1)), dim=1)\n",
    "        )  # (batch_size, output_dim)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "class AttnSeq2Seq(XlitModel):\n",
    "    def __init__(self, model_conf: dict, device: torch.device):\n",
    "        super().__init__(model_conf, device)\n",
    "        self.encoder = Encoder(\n",
    "            model_conf[\"idim\"],\n",
    "            model_conf[\"embed_dim\"],\n",
    "            model_conf[\"hidden_dim\"],\n",
    "            model_conf[\"elayers\"],\n",
    "            model_conf[\"dropout\"],\n",
    "        ).to(device)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            model_conf[\"odim\"],\n",
    "            model_conf[\"embed_dim\"],\n",
    "            model_conf[\"hidden_dim\"],\n",
    "            model_conf[\"dlayers\"],\n",
    "            model_conf[\"dropout\"],\n",
    "        ).to(device)\n",
    "\n",
    "        self.teacher_forcing_ratio = model_conf.get(\"teacher_forcing_ratio\", 0.5)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: Optional[torch.Tensor] = None,\n",
    "        max_len: Optional[int] = None,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        max_len = max_len or self.max_len\n",
    "        batch_size = x.size(0)\n",
    "        y_vocab_size = self.decoder.fc_out.out_features\n",
    "        target_len = y.size(1) if y is not None else max_len\n",
    "\n",
    "        assert target_len is not None, \"max_len must be provided when y is None\"\n",
    "\n",
    "        mask = x != self.pad_token  # (batch_size, seq_len)\n",
    "        encoder_outputs, hidden, cell = self.encoder(x)\n",
    "        outputs = torch.zeros(batch_size, target_len, y_vocab_size, device=self.device)\n",
    "\n",
    "        inp = (\n",
    "            y[:, 0]\n",
    "            if y is not None\n",
    "            else torch.full(\n",
    "                (batch_size,), self.sos_token, dtype=torch.long, device=self.device\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(\n",
    "                inp, hidden, cell, encoder_outputs, mask\n",
    "            )\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            if y is not None:\n",
    "                teacher_force = (\n",
    "                    torch.rand(1, device=self.device) < self.teacher_forcing_ratio\n",
    "                )\n",
    "                inp = torch.where(\n",
    "                    teacher_force.unsqueeze(1), y[:, t].unsqueeze(1), top1.unsqueeze(1)\n",
    "                ).squeeze(1)\n",
    "            else:\n",
    "                inp = top1\n",
    "                if self.eos_token is not None and (inp == self.eos_token).all():\n",
    "                    break\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10359b6",
   "metadata": {},
   "source": [
    "### 4.2 LSTM Seq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d5370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/models/lstm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/lstm.py\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from .base import XlitModel\n",
    "\n",
    "class LSTMSeq2Seq(XlitModel):\n",
    "    def __init__(self, model_conf: dict, device: torch.device):\n",
    "        super().__init__(model_conf, device)\n",
    "        self.embedding = nn.Embedding(model_conf[\"idim\"], model_conf[\"embed_dim\"])\n",
    "        self.encoder = nn.LSTM(\n",
    "            model_conf[\"embed_dim\"],\n",
    "            model_conf[\"hidden_dim\"],\n",
    "            num_layers=model_conf[\"nlayers\"],\n",
    "            dropout=model_conf[\"dropout\"],\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.LSTM(\n",
    "            model_conf[\"embed_dim\"],\n",
    "            model_conf[\"hidden_dim\"],\n",
    "            num_layers=model_conf[\"nlayers\"],\n",
    "            dropout=model_conf[\"dropout\"],\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.output_layer = nn.Linear(model_conf[\"hidden_dim\"], model_conf[\"odim\"])\n",
    "        self.dropout = nn.Dropout(model_conf[\"dropout\"])\n",
    "        self.teacher_forcing_ratio = model_conf.get(\"teacher_forcing_ratio\", 0.5)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor = None, max_len: int = None):\n",
    "        batch_size = x.size(0)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        _, (hidden, cell) = self.encoder(embedded)\n",
    "\n",
    "        if y is not None:\n",
    "            target_len = y.size(1)\n",
    "            outputs = torch.zeros(batch_size, target_len, self.output_layer.out_features, device=self.device)\n",
    "            inp = y[:, 0]\n",
    "        else:\n",
    "            target_len = max_len or self.max_len\n",
    "            assert target_len is not None, \"max_len must be provided when y is None\"\n",
    "            outputs = torch.zeros(batch_size, target_len, self.output_layer.out_features, device=self.device)\n",
    "            inp = torch.full((batch_size,), self.sos_token, dtype=torch.long, device=self.device)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            embedded_input = self.dropout(self.embedding(inp)).unsqueeze(1)\n",
    "            output, (hidden, cell) = self.decoder(embedded_input, (hidden, cell))\n",
    "            output = self.output_layer(output.squeeze(1))\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            if y is not None and torch.rand(1).item() < self.teacher_forcing_ratio:\n",
    "                inp = y[:, t]\n",
    "            else:\n",
    "                inp = top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a210c3",
   "metadata": {},
   "source": [
    "### 4.3 CNN Seq2Seq + PE + GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36a9290e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/models/positional_encoding.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/positional_encoding.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # shape (1, max_len, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88bc0042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/models/cnn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/cnn.py\n",
    "# src/models/cnn.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from .base import XlitModel\n",
    "from .positional_encoding import PositionalEncoding\n",
    "\n",
    "\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.pe = PositionalEncoding(embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T)\n",
    "        embedded = self.embedding(x)            # (B, T, E)\n",
    "        embedded = self.pe(embedded)            # (B, T, E)\n",
    "        embedded = self.dropout(embedded)\n",
    "        conv_input = embedded.transpose(1, 2)   # (B, E, T)\n",
    "        conv_output = self.conv(conv_input)     # (B, H, T)\n",
    "        return conv_output.transpose(1, 2)      # (B, T, H)\n",
    "\n",
    "\n",
    "class ConvAttention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(enc_dim)\n",
    "\n",
    "    def forward(self, query, encoder_outputs):\n",
    "        # query: (B, T_dec, D), encoder_outputs: (B, T_enc, D)\n",
    "        scores = torch.bmm(query, encoder_outputs.transpose(1, 2))  # (B, T_dec, T_enc)\n",
    "        attn_weights = torch.softmax(scores / self.scale, dim=2)\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)          # (B, T_dec, D)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.pe = PositionalEncoding(embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, hidden_dim * 2, kernel_size, padding=kernel_size // 2)\n",
    "        self.attn = ConvAttention(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, encoder_outputs):\n",
    "        # tgt: (B, T)\n",
    "        embedded = self.embedding(tgt)          # (B, T, E)\n",
    "        embedded = self.pe(embedded)\n",
    "        embedded = self.dropout(embedded)\n",
    "        conv_input = embedded.transpose(1, 2)   # (B, E, T)\n",
    "        conv_output = self.conv(conv_input)     # (B, 2H, T)\n",
    "        conv_output = conv_output.transpose(1, 2)  # (B, T, 2H)\n",
    "\n",
    "        H = conv_output.size(-1) // 2\n",
    "        out = conv_output[:, :, :H]\n",
    "        gate = torch.sigmoid(conv_output[:, :, H:])\n",
    "        out = out * gate                        # (B, T, H)\n",
    "\n",
    "        context, _ = self.attn(out, encoder_outputs)  # (B, T, H)\n",
    "        combined = torch.cat([out, context], dim=2)   # (B, T, 2H)\n",
    "        output = self.fc_out(combined)                # (B, T, vocab_size)\n",
    "        return output\n",
    "\n",
    "class CNNSeq2SeqAttn(XlitModel):\n",
    "    def __init__(self, model_conf: dict, device: torch.device):\n",
    "        super().__init__(model_conf, device)\n",
    "        self.encoder = ConvEncoder(\n",
    "            input_dim=model_conf[\"idim\"],\n",
    "            embed_dim=model_conf[\"embed_dim\"],\n",
    "            hidden_dim=model_conf[\"hidden_dim\"],\n",
    "            kernel_size=model_conf[\"kernel_size\"],\n",
    "            dropout=model_conf[\"dropout\"],\n",
    "        )\n",
    "\n",
    "        self.decoder = ConvDecoder(\n",
    "            output_dim=model_conf[\"odim\"],\n",
    "            embed_dim=model_conf[\"embed_dim\"],\n",
    "            hidden_dim=model_conf[\"hidden_dim\"],\n",
    "            kernel_size=model_conf[\"kernel_size\"],\n",
    "            dropout=model_conf[\"dropout\"],\n",
    "        )\n",
    "\n",
    "        self.teacher_forcing_ratio = model_conf.get(\"teacher_forcing_ratio\", 0.5)\n",
    "\n",
    "    def forward(self, x, y=None, max_len=None):\n",
    "        batch_size = x.size(0)\n",
    "        max_len = max_len or self.max_len\n",
    "        target_len = y.size(1) if y is not None else max_len\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        encoder_outputs = self.encoder(x)  # (B, T_src, H)\n",
    "        outputs = torch.zeros(batch_size, target_len, vocab_size, device=self.device)\n",
    "\n",
    "        # Initialize first decoder input\n",
    "        inp = (\n",
    "            y[:, 0] if y is not None\n",
    "            else torch.full((batch_size,), self.sos_token, dtype=torch.long, device=self.device)\n",
    "        )\n",
    "\n",
    "        tgt_tokens = [inp]  # List of tokens to build decoder input\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            decoder_input = torch.stack(tgt_tokens, dim=1)  # (B, t)\n",
    "            logits = self.decoder(decoder_input, encoder_outputs)  # (B, t, vocab)\n",
    "            output_t = logits[:, -1, :]  # (B, vocab)\n",
    "            outputs[:, t] = output_t\n",
    "\n",
    "            top1 = output_t.argmax(1)\n",
    "\n",
    "            if y is not None:\n",
    "                teacher_force = torch.rand(batch_size, device=self.device) < self.teacher_forcing_ratio\n",
    "                inp = torch.where(teacher_force, y[:, t], top1)\n",
    "            else:\n",
    "                inp = top1\n",
    "\n",
    "            tgt_tokens.append(inp)\n",
    "\n",
    "            if self.eos_token is not None and (inp == self.eos_token).all():\n",
    "                break\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283acfb6",
   "metadata": {},
   "source": [
    "### 4.4 Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3210d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/models/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/transformer.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from .base import XlitModel\n",
    "from .positional_encoding import PositionalEncoding\n",
    "    \n",
    "\n",
    "class TransformerSeq2Seq(XlitModel):\n",
    "    def __init__(self, model_conf: dict, device: torch.device):\n",
    "        super().__init__(model_conf, device)\n",
    "\n",
    "        self.embed_dim = model_conf[\"embed_dim\"]\n",
    "        self.encoder_embed = nn.Embedding(model_conf[\"idim\"], self.embed_dim)\n",
    "        self.decoder_embed = nn.Embedding(model_conf[\"odim\"], self.embed_dim)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(self.embed_dim)\n",
    "        self.pos_decoder = PositionalEncoding(self.embed_dim)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=model_conf[\"num_heads\"],\n",
    "            num_encoder_layers=model_conf[\"num_encoder_layers\"],\n",
    "            num_decoder_layers=model_conf[\"num_decoder_layers\"],\n",
    "            dim_feedforward=model_conf[\"dim_feedforward\"],\n",
    "            dropout=model_conf[\"dropout\"],\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.generator = nn.Linear(self.embed_dim, model_conf[\"odim\"])\n",
    "        self.dropout = nn.Dropout(model_conf[\"dropout\"])\n",
    "        self.teacher_forcing_ratio = model_conf.get(\"teacher_forcing_ratio\", 0.5)\n",
    "\n",
    "    def forward(self, src, tgt=None, max_len=None):\n",
    "        batch_size, src_len = src.shape\n",
    "        max_len = max_len or self.max_len\n",
    "        tgt_len = tgt.size(1) if tgt is not None else max_len\n",
    "\n",
    "        src_mask = None\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_len).to(self.device)\n",
    "\n",
    "        src_emb = self.dropout(self.pos_encoder(self.encoder_embed(src)))\n",
    "        memory = self.transformer.encoder(src_emb, mask=src_mask)\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.generator.out_features).to(self.device)\n",
    "\n",
    "        ys = (\n",
    "            tgt[:, 0] if tgt is not None else\n",
    "            torch.full((batch_size,), self.sos_token, dtype=torch.long, device=self.device)\n",
    "        )\n",
    "        ys = ys.unsqueeze(1)\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            tgt_emb = self.dropout(self.pos_decoder(self.decoder_embed(ys)))\n",
    "            out = self.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask[:t, :t])\n",
    "            output = self.generator(out[:, -1])\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "\n",
    "            if tgt is not None:\n",
    "                use_teacher = torch.rand(batch_size, device=self.device) < self.teacher_forcing_ratio\n",
    "                next_input = torch.where(use_teacher, tgt[:, t], top1.squeeze(1))\n",
    "            else:\n",
    "                next_input = top1.squeeze(1)\n",
    "                if self.eos_token is not None and (next_input == self.eos_token).all():\n",
    "                    break\n",
    "\n",
    "            ys = torch.cat([ys, next_input.unsqueeze(1)], dim=1)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab0de7",
   "metadata": {},
   "source": [
    "## 5. Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b34b9e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils/__init__.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile src/utils/__init__.py\n",
    "from .seed import set_seed\n",
    "from .logger import setup_logger\n",
    "from .plot import plot_metrics\n",
    "from .save import save_models, save_best_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f514e714",
   "metadata": {},
   "source": [
    "### 5.1 Set Seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cae7e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils/seed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/seed.py\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f5105",
   "metadata": {},
   "source": [
    "### 5.2 Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d73e916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils/logger.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/logger.py\n",
    "import logging\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def setup_logger(log_file=None, backup_file=None):\n",
    "    logger = logging.getLogger(\"XlitTask\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False\n",
    "    \n",
    "    formatter = logging.Formatter(\"%(asctime)s (%(module)s:%(lineno)d)  %(levelname)s: %(message)s\")\n",
    "\n",
    "    if log_file.exists():\n",
    "        shutil.copy(log_file, backup_file)\n",
    "        log_file.unlink()\n",
    "    \n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.INFO)\n",
    "    fh.setFormatter(formatter)\n",
    "    sh = logging.StreamHandler(sys.stdout)\n",
    "    sh.setLevel(logging.INFO)\n",
    "    sh.setFormatter(formatter)\n",
    "    \n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "        \n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(sh)\n",
    "    \n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9413232",
   "metadata": {},
   "source": [
    "### 5.3 Plot Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d210211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils/plot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/plot.py\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_metrics(image_dir: Path, train_losses, val_losses, val_cers, val_accs):\n",
    "    image_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "    palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "    # Loss plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\", marker='o', color=palette[0])\n",
    "    plt.plot(val_losses, label=\"Val Loss\", marker='s', color=palette[1])\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"Loss\", fontsize=14)\n",
    "    plt.title(\"Training and Validation Loss\", fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(image_dir / \"losses.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # CER plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(val_cers, label=\"Val CER\", marker='^', color=palette[2])\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"Character Error Rate\", fontsize=14)\n",
    "    plt.title(\"Validation CER Over Epochs\", fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(image_dir / \"cer.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Word Accuracy plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(val_accs, label=\"Val Word Accuracy\", marker='D', color=palette[3])\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"Word Accuracy\", fontsize=14)\n",
    "    plt.title(\"Validation Word Accuracy Over Epochs\", fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(image_dir / \"wa.png\", dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07888c3",
   "metadata": {},
   "source": [
    "### 5.4 Save models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c55f4725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils/save.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/save.py\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List\n",
    "import copy\n",
    "import heapq\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def save_models(\n",
    "    exp_dir: Path,\n",
    "    logger: logging.Logger,\n",
    "    current_model: torch.nn.Module,\n",
    "    model_conf: dict,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    max_epoch: int,\n",
    "    avg_train_loss: float,\n",
    "    avg_val_loss: float,\n",
    "    wa_score: float,\n",
    "    best_train_loss: float,\n",
    "    best_val_loss: float,\n",
    "    best_wa: float,\n",
    "    best_models: list,\n",
    "    saved_epochs: set,\n",
    "    n_best: int,\n",
    ") -> Tuple[float, float, float, list, set]:\n",
    "    # Save best training loss\n",
    "    if avg_train_loss < best_train_loss:\n",
    "        best_train_loss = avg_train_loss\n",
    "        torch.save(current_model.state_dict(), exp_dir / \"train.loss.best.pth\")\n",
    "        logger.info(f\"Saved best training loss model at epoch {epoch}.\")\n",
    "\n",
    "    # Save best validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(current_model.state_dict(), exp_dir / \"val.loss.best.pth\")\n",
    "        logger.info(f\"Saved best validation loss model at epoch {epoch}.\")\n",
    "\n",
    "    # Save best word accuracy\n",
    "    if wa_score > best_wa:\n",
    "        best_wa = wa_score\n",
    "        torch.save(current_model.state_dict(), exp_dir / \"wa.best.pth\")\n",
    "        logger.info(f\"Saved best word accuracy model at epoch {epoch}.\")\n",
    "\n",
    "    # Save latest\n",
    "    torch.save(current_model.state_dict(), exp_dir / \"latest.pth\")\n",
    "    logger.info(f\"Saved latest model at epoch {epoch}.\")\n",
    "\n",
    "    # Save top-N best by validation loss using max-heap (invert val_loss)\n",
    "    if epoch not in saved_epochs:\n",
    "        if len(best_models) < n_best:\n",
    "            heapq.heappush(\n",
    "                best_models,\n",
    "                (-avg_val_loss, epoch, copy.deepcopy(current_model.state_dict())),\n",
    "            )\n",
    "            saved_epochs.add(epoch)\n",
    "            torch.save(current_model.state_dict(), exp_dir / f\"{epoch}epoch.pth\")\n",
    "        else:\n",
    "            worst_neg_loss, worst_epoch, _ = best_models[0]\n",
    "            if -avg_val_loss > worst_neg_loss:\n",
    "                removed = heapq.heappushpop(\n",
    "                    best_models,\n",
    "                    (-avg_val_loss, epoch, copy.deepcopy(current_model.state_dict())),\n",
    "                )\n",
    "                saved_epochs.discard(removed[1])\n",
    "                saved_epochs.add(epoch)\n",
    "\n",
    "                # Remove old worst model\n",
    "                worst_path = exp_dir / f\"{removed[1]}epoch.pth\"\n",
    "                if worst_path.exists():\n",
    "                    worst_path.unlink()\n",
    "                    logger.info(f\"Removed evicted model from epoch {removed[1]}.\")\n",
    "\n",
    "                # Save new top model\n",
    "                torch.save(current_model.state_dict(), exp_dir / f\"{epoch}epoch.pth\")\n",
    "\n",
    "    # Save averaged model at end\n",
    "    if epoch == max_epoch:\n",
    "        avg_model = average_model_weights(\n",
    "            best_models, current_model, model_conf, device\n",
    "        )\n",
    "        torch.save(avg_model.state_dict(), exp_dir / \"val.loss.ave.pth\")\n",
    "        logger.info(\"Saved averaged model at the end of training.\")\n",
    "\n",
    "    return best_train_loss, best_val_loss, best_wa, best_models, saved_epochs\n",
    "\n",
    "\n",
    "def average_model_weights(\n",
    "    model_heap: list[tuple[float, int, dict]],\n",
    "    current_model: torch.nn.Module,\n",
    "    model_conf: Dict,\n",
    "    device: torch.device,\n",
    ") -> torch.nn.Module:\n",
    "    n_models = len(model_heap)\n",
    "    assert n_models > 0, \"No models to average.\"\n",
    "    avg_state_dict = copy.deepcopy(model_heap[0][2])\n",
    "\n",
    "    for key in avg_state_dict.keys():\n",
    "        for i in range(1, n_models):\n",
    "            avg_state_dict[key] += model_heap[i][2][key]\n",
    "        avg_state_dict[key] /= n_models\n",
    "\n",
    "    avg_model = copy.deepcopy(current_model)\n",
    "    avg_model.load_state_dict(avg_state_dict)\n",
    "    avg_model.to(device)\n",
    "    return avg_model\n",
    "\n",
    "\n",
    "def save_best_predictions(\n",
    "    exp_dir: Path, xs: List[str], true_texts: List[str], pred_texts: List[str]\n",
    ") -> None:\n",
    "    correct_flags = [\"\" if p == t else \"\" for p, t in zip(pred_texts, true_texts)]\n",
    "    results = [\n",
    "        f\"{x}\\t{t}\\t{p}\\t{c}\"\n",
    "        for x, t, p, c in zip(xs, true_texts, pred_texts, correct_flags)\n",
    "    ]\n",
    "    decode_path = exp_dir / \"decode/wa.best.decode\"\n",
    "    decode_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    decode_path.write_text(\"\\n\".join(results), encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a02b9d",
   "metadata": {},
   "source": [
    "## 6. Xlit Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c604c86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/xlit_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/xlit_task.py\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from jiwer import cer, wer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from .tokenizer import prepare_tokenizers\n",
    "from .data.loader import load_dataloaders\n",
    "from .models import load_model\n",
    "from .utils import (\n",
    "    set_seed,\n",
    "    setup_logger,\n",
    "    plot_metrics,\n",
    "    save_models,\n",
    "    save_best_predictions,\n",
    ")\n",
    "\n",
    "\n",
    "class XlitTask:\n",
    "    def __init__(\n",
    "        self, conf_file: str | Path = \"train.yaml\", ckpt_file: str | Path = None\n",
    "    ) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._load_config(Path(\"conf\") / conf_file)\n",
    "        set_seed(self.conf[\"seed\"])\n",
    "        self.exp_dir = (\n",
    "            Path(\"exp\")\n",
    "            / f\"xlit_train_{self.model_name}_{self.token_type}_{self.lang_pair}\"\n",
    "        )\n",
    "        self.data_dir = self.exp_dir / \"data\"\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.model = self._build_model(ckpt_file).to(self.device)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, ckpt_file: str | Path, conf_file: str | Path = None):\n",
    "        ckpt_file = Path(ckpt_file)\n",
    "        if not ckpt_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Checkpoint file not found: {ckpt_file.as_posix()}\"\n",
    "            )\n",
    "        return cls(conf_file, ckpt_file)\n",
    "\n",
    "    def __call__(self, x: str, max_len: Optional[int] = None) -> str:\n",
    "        return self.infer(x, max_len)\n",
    "\n",
    "    def _load_config(self, conf_path) -> None:\n",
    "        with open(conf_path, \"r\") as f:\n",
    "            content = yaml.safe_load(f)\n",
    "        self.model_name, self.conf = content[\"xlit\"], content[\"xlit_conf\"]\n",
    "        self.token_type = self.conf[\"token_type\"]\n",
    "        self.langx, self.langy = self.conf[\"langx\"], self.conf[\"langy\"]\n",
    "        self.lang_pair = f\"{self.langx}_{self.langy}\"\n",
    "\n",
    "    def _build_model(self, ckpt_file: Optional[str | Path]) -> torch.nn.Module:\n",
    "        self.x_tokenizer, self.y_tokenizer, self.xs, self.ys = prepare_tokenizers(\n",
    "            x_tokens_file=self.data_dir / f\"{self.langx}_{self.token_type}_tokens.txt\",\n",
    "            y_tokens_file=self.data_dir / f\"{self.langy}_{self.token_type}_tokens.txt\",\n",
    "            db_file=self.conf[\"db_file\"],\n",
    "        )\n",
    "        self.conf[\"idim\"], self.conf[\"odim\"] = len(self.x_tokenizer), len(\n",
    "            self.y_tokenizer\n",
    "        )\n",
    "        self.conf[\"pad_token\"] = self.y_tokenizer.tok2idx.get(\"<pad>\", 0)\n",
    "        self.conf[\"sos_token\"] = self.y_tokenizer.tok2idx.get(\"<sos>\", 1)\n",
    "        self.conf[\"eos_token\"] = self.y_tokenizer.tok2idx.get(\"<eos>\", 2)\n",
    "        self.conf[\"max_len\"] = self.conf.get(\"max_len\", 100)\n",
    "        model = load_model(self.model_name, self.conf, device=self.device)\n",
    "        if ckpt_file:\n",
    "            model.load_state_dict(torch.load(ckpt_file, map_location=self.device))\n",
    "        return model\n",
    "\n",
    "    def infer(self, x: str, max_len: Optional[int] = None) -> str:\n",
    "        max_len = max_len or self.conf.get(\"max_len\", 100)\n",
    "        tokenized_x = self.x_tokenizer.encode(x, max_len=max_len)\n",
    "        input_tensor = torch.tensor(tokenized_x).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(\n",
    "                input_tensor,\n",
    "                max_len=max_len,\n",
    "            )\n",
    "        predicted_ids = y_pred.argmax(dim=2)\n",
    "        return self.y_tokenizer.decode(predicted_ids[0].tolist())\n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        (\n",
    "            self.train_loader,\n",
    "            self.val_loader,\n",
    "        ) = load_dataloaders(\n",
    "            self.xs,\n",
    "            self.ys,\n",
    "            self.x_tokenizer,\n",
    "            self.y_tokenizer,\n",
    "            max_len=self.conf[\"max_len\"],\n",
    "            batch_size=self.conf[\"batch_size\"],\n",
    "            val_ratio=self.conf[\"val_ratio\"],\n",
    "            train_file=self.data_dir / f\"train_{self.lang_pair}.txt\",\n",
    "            val_file=self.data_dir / f\"val_{self.lang_pair}.txt\",\n",
    "            seed=self.conf[\"seed\"],\n",
    "        )\n",
    "\n",
    "    def _compute_loss(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return self.criterion(\n",
    "            y_pred[:, 1:].reshape(-1, y_pred.shape[2]),\n",
    "            y_true[:, 1:].reshape(-1),\n",
    "        )\n",
    "\n",
    "    def _train_step(self, batch) -> float:\n",
    "        x, y = batch[\"input\"].to(self.device), batch[\"target\"].to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        y_pred = self.model(x, y)\n",
    "        loss = self._compute_loss(y_pred, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def _val_step(self, batch) -> Tuple[float, List[str], List[str]]:\n",
    "        x, y = batch[\"input\"].to(self.device), batch[\"target\"].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.model(x, y)\n",
    "            loss = self._compute_loss(y_pred, y)\n",
    "            pred_texts = [\n",
    "                self.y_tokenizer.decode(seq.tolist())\n",
    "                for seq in y_pred.argmax(dim=-1).cpu()\n",
    "            ]\n",
    "            true_texts = [self.y_tokenizer.decode(seq.tolist()) for seq in y]\n",
    "        return loss.item(), pred_texts, true_texts\n",
    "\n",
    "    def _init_logs(self):\n",
    "        self.logger.info(\"Data Information:\")\n",
    "        self.logger.info(f\"Batch size: {self.conf['batch_size']}\")\n",
    "        self.logger.info(\n",
    "            f\"[Training] Data size: {len(self.train_loader.dataset)} to {len(self.train_loader)} batches\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"[Validation] Data size: {len(self.val_loader.dataset)} to {len(self.val_loader)} batches\"\n",
    "        )\n",
    "        self.logger.info(\"Token information\")\n",
    "        self.logger.info(\n",
    "            f\"[{self.langx}] Tokenizer loaded with {len(self.x_tokenizer)} tokens.\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"[{self.langy}] Tokenizer loaded with {len(self.y_tokenizer)} tokens.\"\n",
    "        )\n",
    "        total_params = sum(\n",
    "            p.numel() for p in self.model.parameters() if p.requires_grad\n",
    "        )\n",
    "        self.logger.info(f\"Model information:\\n{self.model}\")\n",
    "        self.logger.info(f\"Total trainable parameters: {total_params}\")\n",
    "        self.logger.info(f\"Experiment directory: {self.exp_dir.as_posix()}\")\n",
    "        self.logger.info(f\"Optimizer: {self.optimizer.__class__.__name__}\")\n",
    "        self.logger.info(f\"Loss criterion: {self.criterion.__class__.__name__}\")\n",
    "\n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "        image_dir = self.exp_dir / \"images\"\n",
    "\n",
    "        self.tb_writer = SummaryWriter(log_dir=self.exp_dir / \"tensorboard\")\n",
    "        self.logger = setup_logger(\n",
    "            log_file=self.exp_dir / \"train.log\",\n",
    "            backup_file=self.exp_dir / \"train.old.log\",\n",
    "        )\n",
    "        self._prepare_data()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.conf[\"optim_conf\"][\"lr\"]\n",
    "        )\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self._init_logs()\n",
    "\n",
    "        max_epoch = self.conf.get(\"max_epoch\", 100)\n",
    "        n_best = self.conf.get(\"keep_nbest_models\", 5)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_wa = -float(\"inf\")\n",
    "        best_train_loss = float(\"inf\")\n",
    "\n",
    "        best_models = []\n",
    "        saved_epochs = set()\n",
    "\n",
    "        train_losses, val_losses, val_cers, val_accs = [], [], [], []\n",
    "\n",
    "        self.logger.info(f\"Started training {self.model_name} model on [{self.device}]\")\n",
    "        start_epoch = 1\n",
    "        try:\n",
    "            for epoch in range(start_epoch, max_epoch + 1):\n",
    "                epoch_start_time = time.time()\n",
    "                self.logger.info(f\"Epoch {epoch}/{max_epoch}\")\n",
    "\n",
    "                # Training\n",
    "                self.model.train()\n",
    "                avg_train_loss = sum(\n",
    "                    self._train_step(batch) for batch in self.train_loader\n",
    "                ) / len(self.train_loader)\n",
    "                train_losses.append(avg_train_loss)\n",
    "\n",
    "                self.tb_writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "                self.logger.info(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "                # Validation\n",
    "                self.model.eval()\n",
    "                val_loss = 0.0\n",
    "                pred_texts, true_texts = [], []\n",
    "\n",
    "                val_xs = []\n",
    "                for batch in self.val_loader:\n",
    "                    val_xs.extend(\n",
    "                        [self.x_tokenizer.decode(x.tolist()) for x in batch[\"input\"]]\n",
    "                    )\n",
    "                    loss, preds, trues = self._val_step(batch)\n",
    "                    val_loss += loss\n",
    "                    pred_texts.extend(preds)\n",
    "                    true_texts.extend(trues)\n",
    "\n",
    "                avg_val_loss = val_loss / len(self.val_loader)\n",
    "                cer_score = cer(true_texts, pred_texts)\n",
    "                wa_score = 1 - wer(true_texts, pred_texts)\n",
    "                if best_wa < wa_score:\n",
    "                    save_best_predictions(\n",
    "                        self.exp_dir,\n",
    "                        val_xs,\n",
    "                        true_texts,\n",
    "                        pred_texts,\n",
    "                    )\n",
    "                    self.logger.info(\n",
    "                        f\"Saved best word accuracy predictions at epoch {epoch}.\"\n",
    "                    )\n",
    "\n",
    "                val_losses.append(avg_val_loss)\n",
    "                val_cers.append(cer_score)\n",
    "                val_accs.append(wa_score)\n",
    "\n",
    "                self.tb_writer.add_scalar(\"Loss/Val\", avg_val_loss, epoch)\n",
    "                self.tb_writer.add_scalar(\"CER/Val\", cer_score, epoch)\n",
    "                self.tb_writer.add_scalar(\"Word Accuracy/Val\", wa_score, epoch)\n",
    "                self.logger.info(\n",
    "                    f\"Val Loss: {avg_val_loss:.4f}, CER: {cer_score:.4f}, Word Accuracy: {wa_score:.4f}\"\n",
    "                )\n",
    "\n",
    "                epoch_duration = time.time() - epoch_start_time\n",
    "                elapsed = time.time() - start_time\n",
    "                remaining_epochs = max_epoch - epoch\n",
    "                eta = epoch_duration * remaining_epochs\n",
    "                eta_sec = int(eta)\n",
    "                eta_h, rem = divmod(eta_sec, 3600)\n",
    "                eta_m, eta_s = divmod(rem, 60)\n",
    "                self.logger.info(\n",
    "                    f\"Epoch {epoch} duration: {epoch_duration:.2f} sec | ETA: {eta_h:02d}:{eta_m:02d}:{eta_s:02d}\"\n",
    "                )\n",
    "\n",
    "                best_train_loss, best_val_loss, best_wa, best_models, saved_epochs = (\n",
    "                    save_models(\n",
    "                        self.exp_dir,\n",
    "                        self.logger,\n",
    "                        self.model,\n",
    "                        self.conf,\n",
    "                        self.device,\n",
    "                        epoch,\n",
    "                        max_epoch,\n",
    "                        avg_train_loss,\n",
    "                        avg_val_loss,\n",
    "                        wa_score,\n",
    "                        best_train_loss,\n",
    "                        best_val_loss,\n",
    "                        best_wa,\n",
    "                        best_models,\n",
    "                        saved_epochs,\n",
    "                        n_best,\n",
    "                    )\n",
    "                )\n",
    "                plot_metrics(image_dir, train_losses, val_losses, val_cers, val_accs)\n",
    "                self.logger.info(\n",
    "                    f\"Saved [loss, wa, cer] curves at {image_dir.as_posix()}\"\n",
    "                )\n",
    "        finally:\n",
    "            self.tb_writer.close()\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        h, rem = divmod(int(total_time), 60)\n",
    "        m, s = divmod(rem, 60)\n",
    "        self.logger.info(f\"Training completed in {h:02d}:{m:02d}:{s:02d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d0bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
